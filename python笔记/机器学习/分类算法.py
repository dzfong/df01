

#	监督学习：
#	
#  		分类：目标值离散型
#  				k-近邻算法 、 贝叶斯分类、 决策树与随机森林、 逻辑回归、 神经网络
#  			
# 
# 
# 
#	分类算法的判断依据：
# 		目标值是离散型数据
# 	
# 	
# ----------------------
# 
#  	K- 近邻算法 （基本不使用）
#  	
# ----------------------
# 
#  	
#  		定义：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别
#  		
#  		两个样本的距离 又叫 欧式距离
#  		
#  		K-近邻算法是需要做标准化处理的
#  		比如： a有3个特征(a1,12,13) ; b有3个特征(b1,b2,b3)
#  		
#  		公式  √((𝑎1−𝑏1)^2+(𝑎2−𝑏2)^2+(𝑎3−𝑏3)^2)
#  		
#  		相似的样本，特征之间的值应该都是相近的
#  		
#  		
#  		API:	sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, algorithm='auto')		
#  		属性：	n_neighbors:int 默认为5  k_neighbors查询默认使用的的邻居数
#  				algorithm: {'auto','ball_tree','kd_tree','brute'}, 可用于计算最近邻居的算法，通常使用auto
#  				
#  						
#  		问题：
#  			1、 K值取值问题： 调参 
#  			2、 性能问题：
#  			
#  		
#  		优点： 简单，易于理解， 易于实现， 无需估计参数，无需训练
#  		
#  		缺点： 懒惰算法，对测试样本分类时的计算量大，内存开销大
#  			   必须指定K值，K值选择不当则分类精度不能保证
#  		
#  		使用场景： 小数据场景， 几千~ 几万样本，具体场景具体业务去测试
#  		
#  		





#-------------------------------
#
# 	朴素贝叶斯算法
# 	
#-------------------------------
#
#
# 